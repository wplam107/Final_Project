{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T00:38:52.831355Z",
     "start_time": "2020-04-06T00:38:52.815548Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T01:26:41.107855Z",
     "start_time": "2020-04-06T01:26:41.083601Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('articles.p', 'rb')      \n",
    "df = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T01:26:41.505167Z",
     "start_time": "2020-04-06T01:26:41.497329Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_indices = df.loc[df['date'] < pd.Timestamp(2019, 3, 15)].index\n",
    "df.drop(index=drop_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T01:26:42.009973Z",
     "start_time": "2020-04-06T01:26:41.991896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop update articles and investing articles\n",
    "df = df.loc[df['headline'].map(lambda x: re.search(r'UPDATE', x)).isna()]\n",
    "df = df.loc[df['headline'].map(lambda x: re.search(r'US STOCKS', x)).isna()]\n",
    "df = df.loc[df['url'].map(lambda x: re.search(r'investing', x)).isna()]\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(columns='index', inplace=True)\n",
    "df.drop(columns='url', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T01:26:45.348558Z",
     "start_time": "2020-04-06T01:26:45.339423Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_words(text):\n",
    "    text = re.sub(r'U\\.S\\.', 'US', text)\n",
    "    text = re.sub(r'U\\.S\\.A\\.', 'US', text)\n",
    "    text = re.sub(r'US', 'USA', text)\n",
    "    text = re.sub(r'Kongs', 'Kong', text)\n",
    "    text = re.sub(r'Hong Kong', 'HongKong', text)\n",
    "    text = re.sub(r'U\\.K\\.', 'UK', text)\n",
    "    text = re.sub(r'Mr\\.', 'MR', text)\n",
    "    text = re.sub(r'Mrs\\.', 'MRS', text)\n",
    "    text = re.sub(r'Ms\\.', 'MS', text)\n",
    "    text = re.sub(r'\\.\\.\\.', '', text)\n",
    "    text = re.sub(r'U.S-China', 'US-China', text)\n",
    "    text = text.replace('Co.', 'Co')\n",
    "    text = text.replace('\\xa0', '')\n",
    "    text = text.replace('.\"', '\".')\n",
    "    text = text.replace('immediatelywith', 'immediately with')\n",
    "    text = text.replace('theOfficeof', 'the Office of')\n",
    "    text = text.replace('theCommissionerof', 'the Commissioner of')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T01:26:47.722668Z",
     "start_time": "2020-04-06T01:26:47.663758Z"
    }
   },
   "outputs": [],
   "source": [
    "df['body'] = df['body'].map(replace_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T02:04:01.142281Z",
     "start_time": "2020-04-06T02:04:01.129869Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_words(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in stop_words:\n",
    "            result.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T02:04:01.464231Z",
     "start_time": "2020-04-06T02:04:01.456668Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in stop_words:\n",
    "            result.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T02:04:02.458989Z",
     "start_time": "2020-04-06T02:04:02.455062Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_all(listy):\n",
    "    results = [ preprocess_sentence(text) for text in listy ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T01:38:22.567867Z",
     "start_time": "2020-04-06T01:38:18.471862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize body sentences, keep first 10 sentences\n",
    "df['sentence_tokens'] = df['body'].map(lambda x: sent_tokenize(x)[:10])\n",
    "\n",
    "# Preprocess sentences\n",
    "df['sentence_tokens'] = df['sentence_tokens'].map(preprocess_all)\n",
    "\n",
    "# Preprocess words\n",
    "df['word_tokens'] = df['body'].map(preprocess_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T23:18:06.249958Z",
     "start_time": "2020-04-05T23:18:05.883529Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(pro_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T23:18:09.177812Z",
     "start_time": "2020-04-05T23:18:09.150762Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T23:18:10.623138Z",
     "start_time": "2020-04-05T23:18:10.511793Z"
    }
   },
   "outputs": [],
   "source": [
    "bow_corpus = [ dictionary.doc2bow(doc) for doc in pro_body ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T23:18:11.957256Z",
     "start_time": "2020-04-05T23:18:11.916038Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T23:18:26.295869Z",
     "start_time": "2020-04-05T23:18:18.636847Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'Users/waynelam/Documents/DevStuff/mallet-2.0.8/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T23:22:42.986344Z",
     "start_time": "2020-04-05T23:22:42.982894Z"
    }
   },
   "outputs": [],
   "source": [
    "def vader_analysis(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T23:46:45.942702Z",
     "start_time": "2020-04-05T23:46:45.938561Z"
    }
   },
   "outputs": [],
   "source": [
    "def polarity(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment[0]\n",
    "\n",
    "def subjectivity(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
